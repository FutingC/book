{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra in PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is a popular package for developing models for deep learning.  In this section, we'll look at its linear algebra capabilities.\n",
    "\n",
    "Even if you are not doing deep learning, you can use PyTorch for linear algebra.  One of the nice things about PyTorch is that it makes it easy to take advantage of GPU hardware, which is very efficient at certain operations.\n",
    "\n",
    "You can find [PyTorch tutorials](https://pytorch.org/tutorials/) on the official website as well as [documentation](https://pytorch.org/docs/stable/index.html).  Note these tutorials are focused on deep learning.  This section will focus on the linear algebra capabilities.\n",
    "\n",
    "When you install PyTorch, use the `pytorch` channel in conda.\n",
    "\n",
    "```bash\n",
    "conda install pytorch -c pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "PyTorch tensors are just multi-dimensional arrays.  You can go back and forth between these and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78570125, 0.24589094],\n",
       "       [0.33821079, 0.36772354]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.rand(2,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7857, 0.2459],\n",
       "        [0.3382, 0.3677]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.Tensor(A)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put a tensor on gpu, use `cuda()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (999) : unknown error at /opt/conda/conda-bld/pytorch_1595629395347/work/aten/src/THC/THCGeneral.cpp:47",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-543f4698daf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBcpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycourse/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m             raise AssertionError(\n\u001b[1;32m    189\u001b[0m                 \"libcudart functions unavailable. It looks like you have a broken build?\")\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (999) : unknown error at /opt/conda/conda-bld/pytorch_1595629395347/work/aten/src/THC/THCGeneral.cpp:47"
     ]
    }
   ],
   "source": [
    "Bcuda = B.cuda()\n",
    "print(Bcuda)\n",
    "Bcpu = B.cpu()\n",
    "print(Bcpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8763, 0.2339],\n",
       "        [0.3822, 0.8602]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Functions\n",
    "\n",
    "PyTorch provides access to a variety of BLAS and LAPACK-type routines - see [documentation here](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations).  These do not follow the BLAS/LAPACK naming conventions\n",
    "\n",
    "[`torch.addmv`](https://pytorch.org/docs/stable/generated/torch.addmv.html#torch-addmv) is roughly equivalent to `axpy`, and performs $Ax + y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5.9996, -13.2232,  -7.8537,  -1.2397,   7.0935,   6.2512, -21.6037,\n",
       "          3.0582,   0.4784,  -0.8764,  10.3109, -11.9672,  -5.3819, -14.5128,\n",
       "         10.0314, -11.4222,   5.6935,  -4.2006,   0.8691, -15.6624,  -9.7964,\n",
       "        -10.4048,  -5.1958,   4.7203, -13.0604,  -2.9627,  -3.9022, -11.1822,\n",
       "         -0.7486,  -1.0978,   2.1650,  16.4536,  13.0700,   5.6372, -11.6202,\n",
       "          3.9144,  -9.4889,  18.1967,  -1.6189,  13.6554,   1.9952,  -5.2764,\n",
       "         15.7601,  -7.3246,  -6.6940,  -0.4277,   0.4601, -20.7658,   0.2272,\n",
       "         -8.2151,  22.4861, -11.9605,  -5.5331, -15.1731,  -5.3083,  -2.2013,\n",
       "         21.9076,   1.1307,  -5.8735,   0.2406,  25.1729,  16.2230,   5.1621,\n",
       "          2.1377,   0.6817,   1.8963,   6.3730,  -7.3287,  12.3657, -12.5891,\n",
       "          5.1580,   7.1713,   9.3990,   5.3373,  -4.4808, -18.1150,  -6.8875,\n",
       "          2.9506,  -5.0131,   3.2138,  -4.8178,   8.6255,   4.5671,  16.8259,\n",
       "         -7.8197,  -0.9449,  13.7888,   6.7392, -15.4357,  20.3459,  -7.4387,\n",
       "          6.7937,   7.7861,  11.5421,  -7.9804, -15.6283,  12.7771,  -8.7124,\n",
       "         15.2281,   3.4295], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 100\n",
    "n = 100\n",
    "device = torch.device('cuda') # 'cuda' or 'cpu'\n",
    "\n",
    "Anp = np.random.randn(m,n)\n",
    "xnp = np.random.randn(n)\n",
    "ynp = np.random.randn(m)\n",
    "\n",
    "A = torch.Tensor(Anp).to(device=device)\n",
    "x = torch.Tensor(xnp).to(device=device)\n",
    "y = torch.Tensor(ynp).to(device=device)\n",
    "\n",
    "z = torch.addmv(y, A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the timing difference between CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "CPU times: user 239 µs, sys: 59 µs, total: 298 µs\n",
      "Wall time: 302 µs\n",
      "device = cuda\n",
      "CPU times: user 51 µs, sys: 0 ns, total: 51 µs\n",
      "Wall time: 54.4 µs\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "n = 1000\n",
    "\n",
    "Anp = np.random.randn(m,n)\n",
    "xnp = np.random.randn(n)\n",
    "ynp = np.random.randn(m)\n",
    "\n",
    "for device in ['cpu', 'cuda']:\n",
    "    print(f\"device = {device}\")\n",
    "    A = torch.Tensor(Anp).to(device=device)\n",
    "    x = torch.Tensor(xnp).to(device=device)\n",
    "    y = torch.Tensor(ynp).to(device=device)\n",
    "    z = torch.addmv(y, A, x)\n",
    "\n",
    "    %time z = torch.addmv(y, A, x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`torch.mv`](https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv) performs matrix-vector products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "CPU times: user 207 µs, sys: 52 µs, total: 259 µs\n",
      "Wall time: 262 µs\n",
      "device = cuda\n",
      "CPU times: user 35 µs, sys: 0 ns, total: 35 µs\n",
      "Wall time: 37.9 µs\n"
     ]
    }
   ],
   "source": [
    "Anp = np.random.randn(m,n)\n",
    "xnp = np.random.randn(n)\n",
    "\n",
    "for device in ['cpu', 'cuda']:\n",
    "    print(f\"device = {device}\")\n",
    "    A = torch.Tensor(Anp).to(device=device)\n",
    "    x = torch.Tensor(xnp).to(device=device)\n",
    "    y = torch.mv(A, x)\n",
    "\n",
    "    %time y = torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[torch.mm](https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm) performs matrix-matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "CPU times: user 31 ms, sys: 690 µs, total: 31.7 ms\n",
      "Wall time: 8.24 ms\n",
      "device = cuda\n",
      "CPU times: user 27 µs, sys: 7 µs, total: 34 µs\n",
      "Wall time: 38.6 µs\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "n = 1000\n",
    "\n",
    "Anp = np.random.randn(m,n)\n",
    "Bnp = np.random.randn(n, n)\n",
    "\n",
    "for device in ['cpu', 'cuda']:\n",
    "    print(f\"device = {device}\")\n",
    "    A = torch.Tensor(Anp).to(device=device)\n",
    "    B = torch.Tensor(Bnp).to(device=device)\n",
    "    C = torch.mm(A, B) # run once to warm up\n",
    "\n",
    "    %time C = torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch operations\n",
    "\n",
    "Where PyTorch (and GPUs in general) really shine are in **batch** operations.  We get extra efficiency if we do a bunch of multiplications with matrices of the same size.\n",
    "\n",
    "For matrix-matrix multiplcation, the function is [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm)\n",
    "\n",
    "Because tensors are row-major, we want the batch index to be the first index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "CPU times: user 192 ms, sys: 13.8 ms, total: 206 ms\n",
      "Wall time: 51.8 ms\n",
      "device = cuda\n",
      "CPU times: user 239 µs, sys: 53 µs, total: 292 µs\n",
      "Wall time: 298 µs\n"
     ]
    }
   ],
   "source": [
    "n = 512 # matrix size\n",
    "k = 32 # batch size\n",
    "\n",
    "for device in ['cpu', 'cuda']:\n",
    "    print(f\"device = {device}\")\n",
    "    A = torch.randn(k, n, n).to(device=device)\n",
    "    B = torch.randn(k, n, n).to(device=device)\n",
    "    C = torch.bmm(A, B) # run once to warm up\n",
    "\n",
    "    %time C = torch.bmm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Linear Algebra\n",
    "\n",
    "PyTorch also supports sparse tensors in [`torch.sparse`](https://pytorch.org/docs/stable/sparse.html).  Tensors are stored in [COOrdinate format](sparse.html#coordinate-format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 3.],\n",
       "        [4., 0., 5.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.LongTensor([[0, 1, 1],\n",
    "                      [2, 0, 2]])\n",
    "v = torch.FloatTensor([3, 4, 5])\n",
    "torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycourse)",
   "language": "python",
   "name": "pycourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
